---
title: NJU SE 机器学习
date: 2024-01-05
category: [Knowledge, Machine Learning]
toc_number: false
---

重点整理复习版

图片无法完全加载，请访问[语雀文档](https://www.yuque.com/u34135790/laiu5z/ensdbirr9p0xpaac)

# 1 基础概念
## 1.1 什么是机器学习
通过数据训练的学习算法
## 1.2 监督，无监督，自监督，半监督

- 监督：利用大量标注数据（真实标签）训练模型
- 无监督：不依赖任何人工标注标签（聚类/降维/离散点检测）
- 自监督：标注来源于数据本身（对比学习）
- 半监督：深度学习领域...?
## 1.3 欠拟合和过拟合
过拟合：病态问题，学习能力强，过度计算导致模型泛化能力下降。在训练集上表现好（训练误差小），在测试集上表现差。
欠拟合：模型复杂度低，不能在训练集上实现足够低的误差，学习不到数据的规律。
## 1.4 评价学习算法的指标
混淆矩阵
列：预测为该类别的数目；行：实际为该类别的数目
【二分类】

|    |  postive |  negative  |
| --- | --- | --- |
| positive | TP | FN |
| negative | FP | TN |

- 精度/准确率：$ACC = \frac{TP+TN}{TP+FP+FN+TN}$（错误率：1-acc）
- 查准率：$Precision = \frac{TP}{TP+FP}$
- 查全率：$Recall = \frac{TP}{TP+FN}$
- F1：$F1 = 2\frac{precision*recall}{precision+recall}$

## 1.5 没有免费的午餐
没有天生优越的学习器，只有相对好的建模，充分利用了与问题相关的先验知识模型才是最优的
## 1.6 距离度量的计算方式
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml01.png?raw=true)
### 1.6.1 曼哈顿距离
向量在每一维度上的相对距离和，即$d = \Sigma_{k=1}^d|x_{ik}-x_{jk}|$
在二维平面上，两个点的曼哈顿距离表现为x方向上距离和y方向上距离的和
### 1.6.2 切比雪夫距离
各维度坐标数值差绝对值的最大值
### 1.6.3 马氏距离
$M$为协方差矩阵的逆矩阵，要求**总体样本数大于样本的维数**，否则得到的总体样本协方差矩阵逆矩阵不存在
【如何计算协方差矩阵】
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml02.png?raw=true)
一行是一个样本，每一列是一个随机变量
### 1.6.4 闵可夫斯基距离
$d(x_i,x_j) = (\Sigma_{u=1}^n|x_{iu}-x_{ju}|^p)^{\frac{1}{p}}$
p = 1：曼哈顿距离
p = 2：欧氏距离
p = $\infin$：切比雪夫距离
# 2 KNN
k nearest neighbor classifier
懒惰学习算法，不需要学习成本，需要存储数据成本
## 2.1 算法流程

1. 计算所有测试样本和所有训练样本的距离d
2. （距离升序排序？）
3. 针对每一个测试样本，选择k个最近的训练样本
4. 采用投票法为测试样本选定分类标签

（如果是回归：将$\frac{1}{d}$作为权重，取k个近邻标签的加权平均）
测试阶段时间复杂度$O(nd+nlogk)$：nlogk的解释，k个数的最小堆
## 2.2 k值影响

- 取奇数，避免平局
- k较小，对噪声敏感，模型复杂，容易过拟合；k较大，对噪声不敏感，容易欠拟合
## 2.3 核平滑
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml03.png?raw=true)
核平滑方法是指使用核函数来计算测试样本的标签值（在回归中）
## 2.4 降低计算
### 2.4.1 维诺图
适合维度2-5：划分区块（维诺单元），每个维诺单元都是一个凸多面体
【2维维诺图】
计算维诺图：$O(nlogn)$算法决定
测试：$O(logn)$使用空间搜索树确定维诺单元（类似于平衡二叉树）
### 2.4.2 KD-Tree
适合特征维度6-30：相当于二叉树，用来划分空间
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml04.png?raw=true)
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml05.png?raw=true)
【构造】

1. 计算x、y方向上的方差，选择方差大的轴进行划分
2. 选取选定方向上数据的中位数进行划分
3. 在划分出的新区域递归以上步骤

【测试】
kd树搜索
1. 二叉搜索：从根节点开始，以递归的方式从树的顶端向下移动
2. 当到达一个叶子节点，即得到最邻近的近似点，判断其是否为最优，并保存为“当前最优”
3. 回溯：对整棵树进行递归，并对每个节点执行以下操作
  - 如果当前节点比"当前最优"更近，替换为新的"当前最优"
  - 判断分割平面的另一侧是否存在比"当前最优"更优的点。构造一个超球面，球心为查询点，半径为与当前最优的距离
      - 如果超球面跟超平面相交，则有可能存在更优的点；按照相同的搜索过程从当前节点向下移动到树的另一个分支以寻找更近的点
      - 如果超球面跟超平面不相交，则沿着树继续往上走，当前节点的另个分支则被排除
4. 当算法为根节点完成整个过程时，算法结束
### 2.4.3 降维
参考降维
### 2.4.4 ANN 近似最近邻
搜索可能是近邻的数据项，牺牲精度
### 2.4.5 哈希
把任意长度输入映射成固定长度输出
# 3 聚类
聚类：数据对象的集合
聚类算法：根据给定的相似性评价标准将一个数据集合划分成几个聚类
相似性度量+聚类准则
## 3.1 聚类算法
### 3.1.1 试探法
凭借感觉/经验针对实际问题定义阈值 -> 最近邻规则（某种距离计算方式+对应阈值）
误差：与聚类中心（均值）距离平方和
初始点、样本次序和阈值都会影响
【最大最小距离法】
选择与确定的聚类中心最远的点作为新的中心，预先选定聚类中心
已经选定多个后添加新的中心：每一个样本，计算到所有中心的最小值，选择所有样本中的最大值，如果大于$\theta ||z_1-z_2||$选定，否则选取过程结束
### 3.1.2 系统聚类
计算类之间的距离，合并新类
【类之间距离】

- 最短距离
- 最长距离
- 类平均距离
### 3.1.3 动态聚类法
#### 3.1.3.1 K-means

1. 确定聚类数量k
2. 初始化k个聚类中心：随机选择k个样本点
3. 对每个样本点计算最近聚类距离
4. 更新聚类中心（平均值）
5. 没有聚类中心移动：停止

**【k-means++】**

- 随机选1个样本点作为初始聚类中心
- 每一个样本点被选为下一个聚类中心的概率$\frac{D(x)^2}{\Sigma D(x)^2}$，$D(x)$表示和所有中心的最短距离
- 使用轮盘法选择聚类中心
#### 3.1.3.2 ISODATA
分裂+合并
根据样本到聚类中心分配样本，如果某一类样本数少于n，合并；类别数目小于K0/2，分裂；类别数目大于2K0，合并；
以平均中心作为聚类中心，更新中心后重复以上操作
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml06.png?raw=true)
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml07.png?raw=true)
## 3.2 如何评价聚类好坏
类内距离小，类间距离大
### 3.2.1 Compactness - CP
${CP_i} = \frac{1}{|\Omega_i|}\Sigma_{x_j \in \Omega_i}||x_i-w_i||$
$\Omega_i$表示某一个类，$w_i$表示聚类中心
紧密度计算类内距离：越小类内越紧凑
### 3.2.2 Separation - SP
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml08.png?raw=true)

间隔度越大越分散
### 3.2.3 Davies-Bouldin Index - DBI
戴维森堡丁指数/分类适确性指标
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml09.png?raw=true)
缺点：欧氏距离对于环状分布聚类评价很差
### 3.2.4 Dunn Validity Index - DVI
邓恩指数
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml10.png?raw=true)
对离散点聚类测评很高，对环状分布测评效果差
### 3.2.5 其他评价指标
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml11.png?raw=true)
# 4 树学习
## 4.1 符号学习
### 4.1.1 推理
推理（正向/反向）
归纳推理：前件为真，后件未必为真
### 4.1.2 概念学习
给定样例判断每个样例是否属于某个概念
#### 4.1.2.1 实例空间与假设空间
【实例空间】所有可能样例集合
【假设空间】除了所有样例，还可能涉及到未知、空等情况
#### 4.1.2.2 泛化和特化
样例$h_i$和$h_j$预测的类别相同，$h_j$包含的实例数更多
【泛化】$h_j \ge_g h_k$
【特化】$h_k \ge_s h_j$
#### 4.1.2.3 Find-S
寻找极大特殊假设
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml12.png?raw=true)
满足：同一种结果，在这种属性上取值一致
【列表消除算法】
列出所有假设空间，消除不符合实例的假设（实际中不太可能）
## 4.2 变型空间
### 4.2.1 概念
【一致】一个假设和样例集合一致：$h(x) = c(x)$，$h(x)$表示假设函数，$c(x)$表示实例结果
变型空间定义：假设空间和样例集合一致的所有假设构成子集
【极大泛化】
【极大特化】
### 4.2.2 表示定理
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml13.png?raw=true)
h：其中一种假设（布尔函数）
### 4.2.3 候选消除算法
正例用于S泛化，搜索S集合；反例用于G特化，缩小G集合
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml14.png?raw=true)
有关find-s和候选消除算法讲解很细致易懂的博客[概念学习和一般到特殊序 - WTSRUVF - 博客园](https://www.cnblogs.com/WTSRUVF/p/15680429.html)
### 4.2.4 Find-S 与候选消除算法区别

- find-s找到适合所有正例的假设，候选消除算法维护一组能够区分正例和负例的一致性假设
- find-s从最特殊的假设开始进行泛化，候选消除算法从一般的泛化集合和最特殊的特化集合开始
- find-s只考虑正例，不考虑负例；候选消除算法中正负例都会影响假设集合
- find-s使用专门的搜索策略找到最特定的假设；候选消除算法搜索更全面
## 4.3 归纳偏置
某种形式的预先假定（前提）
## 4.4 决策树
归纳偏置：优先选择较小树
【优点】

- 容错能力好，健壮性高
- 可解释性强
- 不需要数据预处理
- 可以处理多维度输出分类问题

【缺点】

- 容易过拟合
- 样本改动会剧烈影响树结构
- NP问题容易陷入局部最优
### 4.4.1 ID3算法
#### 4.4.1.1 算法流程

1. 创建root结点
2. 如果所有样本属性一致，返回叶子节点；如果未划分的属性为空，选择所有样本中最普遍的标签（目标属性）；如果所有样本类别相同，返回叶子节点。【递归停止条件】
3. 否则，选择分类样本能力最好的属性，依据属性的每个可能值划分样本递归执行
#### 4.4.1.2 如何选择最佳属性
##### 4.4.1.2.1 熵
目标属性为布尔值
$Entropy(S) = -p_+log(p_+)-p_-log(p_-)$
##### 4.4.1.2.2 信息增益
使用A属性分割样例，导致期望熵降低（选择maxGain）
$G(S,A) = Entropy(S) - \Sigma_{v\in Values(A)} \frac{|S_v|}{|S|} Entropy(S_v)$
#### 4.4.1.3 特点
假设空间包含所有决策树，一次遍历，不进行回溯（局部最优）
对错误样例不敏感，不适用于增量处理
对可取值数目多的属性**有偏好**
### 4.4.2 奥卡姆剃刀原理
如果对同一现象有两种不同假说，应该采取比较简单的一种
优先选择拟合数据最简单的假设
### 4.4.3 C4.5算法
【**信息增益比】**
$GainRate(S,A) = \frac{Gain(S,A)}{Entropy_A(S)}$
$Entropy_A(S) = -\Sigma_{v\in Values(A)} \frac{|S_v|}{|S|} log(\frac{|S_v|}{|S|})$
除以属性的熵
### 4.4.4 CART算法
#### 4.4.4.1 Gini系数
k个类中，样本属于第k类的概率为$p_k = \frac{|C_k|}{|D|}$
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml15.png?raw=true)
二分类问题gini系数![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml16.png?raw=true)
gini系数代表模型的纯度，越小越好
【属性划分后计算基尼系数】
![](https://github.com/AliceRayLu/AliceRayLu.github.io/blob/main/source/_posts/knowledge/machine%20learning/assets/ml17.png?raw=true)
#### 4.4.4.2 回归
对某一属性A，找到一个点s使得s左右两边数据集各自均方差（标准差？）相加最小
选择和最小的属性
s为经过排序后，某两个相邻样本的平均数
【回归输出】
采用叶子节点的均值或中位数作为预测结果
### 4.4.5 剪枝
#### 4.4.5.1 后剪枝
从决策树底部剪去一些子树，在独立验证集上测试选择最优子树
剪去子树：变成占比高的叶子节点标签
#### 4.4.5.2 最小化子树损失函数
![]()
# 5 集成学习
## 5.1 原理
多个分类器集成在一起以提高分类准确率
集成方法包括多数投票法等等
### 5.1.1 准确性计算
假设每个二分类器精度为p，且相互独立
继承后T个二分类器的分类器的精度为$\Sigma_{k=\frac{T}{2}+1}^T C_T^k p^k(1-p)^{T-k}$
T足够大时近似二项分布，要求每个分类器的准确率要在50%以上
### 5.1.2 bias & variance tradeoff
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704172680446-a924c53c-1998-4c2c-b3ed-a89c44f3e88e.png#averageHue=%23f7f4f4&clientId=u01548630-8dd6-4&from=paste&height=395&id=u67a0d171&originHeight=592&originWidth=1159&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=244891&status=done&style=none&taskId=u50b1b782-9c40-457a-a419-6aa596d9a27&title=&width=772.6666666666666)
### 5.1.3 基本策略
#### 5.1.3.1 回归问题

- 简单平均
- 加权平均
#### 5.1.3.2 分类问题
投票法

- 绝对多数
- 相对多数
- 加权投票
## 5.2 Bagging
bootstrap aggregating
### 5.2.1 基本原理
有放回的采样方法
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704173271777-3c05192b-1f4b-4289-8c55-f730cd46c757.png#averageHue=%23f9f8f8&clientId=u01548630-8dd6-4&from=paste&height=343&id=u6fb20d90&originHeight=514&originWidth=1044&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=129105&status=done&style=none&taskId=u76c0d4e4-5e70-4b05-a809-7b1e66d560b&title=&width=696)
### 5.2.2 优点&缺点
【优点】
并行式集成学习，降低分类器方差，改善泛化
【缺点】
基学习器高bias会影响集成后学习器的高bias
集成后损失可解释性
### 5.2.3 随机森林RF
有放回抽样 -> 生成随机树（随机抽取特征） -> 使用没有被抽到的样本进行测试（多数投票/平均）
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704174247703-7751d448-70a8-48ed-9771-6d7697649422.png#averageHue=%23f8f7f6&clientId=u01548630-8dd6-4&from=paste&height=412&id=u26493a0a&originHeight=618&originWidth=1143&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=304986&status=done&style=none&taskId=u0cd1c0c6-5f17-4e24-8003-6e113cac6ea&title=&width=762)
【特点】

- 差异性：每棵树使用特征不同
- 缓解维度灾难：抽取一部分特征来生成决策树
- 可并行化
- 训练测试无需划分：有30%左右数据没有被采样
- 稳定：投票/平均
## 5.3 Boosting
### 5.3.1 基本原理
**probably approximately correct(PAC) - **概率近似正确
【强可学习】如果存在一个多项式的学习算法能够学习，并且正确率很高
【弱可学习】多项式的学习算法，但是正确率仅比随机猜测略好
【PAC学习理论】强学习器和弱学习器是等价的，可以通过**提升**将弱学习器转化为强学习器
### 5.3.2 AdaBoost
Adaptive Boost，二分类学习算法
#### 5.3.2.1 基本思想
改变训练数据的概率分布，反复学习，得到一系列的弱学习器组合形成一个强分类器
提高错误分类样本权值，降低正确分类权值
集成时加权投票：错误率小的分类器权重高，错误率大权重低
#### 5.3.2.2 计算
第k个弱分类器$G_k(x)$在训练集上加权分类的错误率为$e_k = \Sigma_{i=1}^m w_{k,i}I(G_k(x_i) \neq y_i)$，其中$w_{k,i}$表示第k个分类器输出i个样本权重
得到第k个分类器$G_k(x)$投票权重系数$\alpha_k = \frac{1}{2} log\frac{1-e_k}{e_k}$
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704175526522-9a4a866f-da7f-4996-bf0b-013f92380fe9.png#averageHue=%23fbf6f6&clientId=u01548630-8dd6-4&from=paste&height=383&id=ud93c9b8f&originHeight=574&originWidth=1068&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=149737&status=done&style=none&taskId=u9ed5c6b5-93d9-40b3-87b2-e4d09334e89&title=&width=712)
【可视化表现】
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704175728899-8ff4bbc0-fb74-4e5b-9b54-333c88a5b7d2.png#averageHue=%23f6eae3&clientId=u01548630-8dd6-4&from=paste&height=435&id=u9d190930&originHeight=652&originWidth=1068&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=179445&status=done&style=none&taskId=u1e71a3cd-704c-4928-a42d-d64c67ee0b3&title=&width=712)
#### 5.3.2.3 解释
##### 5.3.2.3.1 加法模型
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704176078575-250663a0-17b6-4957-b667-c201924d7c1b.png#averageHue=%23f8f8f8&clientId=u01548630-8dd6-4&from=paste&height=355&id=ua602e401&originHeight=532&originWidth=970&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=88971&status=done&style=none&taskId=u07b69d05-a29e-4d21-b4d5-e9cf1e17163&title=&width=646.6666666666666)
损失函数可以写作$L(y_i,f(x))$，f(x)为上述表达式，这个函数优化起来十分复杂，可以采用前向分步算法
##### 5.3.2.3.2 前向分步算法
每一步只学习一个基函数及其系数
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704176266076-0db4ff11-977c-47e6-b8e6-9a466f104e8b.png#averageHue=%23f7f7f7&clientId=u01548630-8dd6-4&from=paste&height=391&id=u0a766c18&originHeight=586&originWidth=1033&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=103412&status=done&style=none&taskId=u2562af1b-3bf7-4fba-aae7-194e0637ff6&title=&width=688.6666666666666)
adaboost是前向分步算法的特例，损失函数是指数函数
##### 5.3.2.3.3 分类器与损失函数
【最终分类器】
$f(x) = \Sigma_{k=1}^{K} \alpha_k G_k(x_i)$
【损失函数】
$L(y,f(x)) = exp(-yf(x))$
### 5.3.3 其他boosting算法
#### 5.3.3.1 boosting tree
平方误差损失函数
#### 5.3.3.2 GBDT
梯度提升树，使用损失函数和分类器分类结果的偏导求得梯度作为残差，拟合回归树
#### 5.3.3.3 XGBoost
extreme gradient boosting
GBDT的高效实现，使用二阶泰勒展开做近似
## 5.4 Stacking
k-fold
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704177200490-7a4c7e9d-5f29-4978-a1f9-56d1019e72d8.png#averageHue=%23f7f5f3&clientId=u01548630-8dd6-4&from=paste&height=340&id=u35cbeb46&originHeight=510&originWidth=1084&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=213720&status=done&style=none&taskId=uf3e2acd6-c32e-4927-aac7-2a66db78f8a&title=&width=722.6666666666666)
# 6 概率学习
## 6.1 基本数学概念
### 6.1.1 张量tensor
一个泛化的实数构成的n维数组
### 6.1.2 带/不带约束的数学优化问题
#### 6.1.2.1 带约束
比如拉格朗日（见SVM部分）
#### 6.1.2.2 不带约束
**【最小二乘问题】**least-squares
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704241201180-ddc723a1-8230-4900-9cba-4bdf63c5a335.png#averageHue=%23f9f9f8&clientId=u11c3f106-9eb4-4&from=paste&height=273&id=u29350ac6&originHeight=409&originWidth=952&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=97631&status=done&style=none&taskId=u2a1224b8-326f-4a48-b3d1-5e5d478a571&title=&width=634.6666666666666)
### 6.1.3 凹凸函数
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704241826448-6386f48f-0441-42dc-a797-5096e10e0855.png#averageHue=%23fafafa&clientId=u11c3f106-9eb4-4&from=paste&height=197&id=uf190090c&originHeight=669&originWidth=1114&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=131799&status=done&style=none&taskId=ua4c27820-97fc-4d0d-942d-e42f0b3f205&title=&width=328.3333435058594)![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704241953365-a816085e-5a6f-48b6-b1b3-b468a31633b6.png#averageHue=%23fafafa&clientId=u11c3f106-9eb4-4&from=paste&height=196&id=u3a611acf&originHeight=706&originWidth=1089&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=112475&status=done&style=none&taskId=u7171875e-9c16-4b68-8fe7-f8bd87b8517&title=&width=302.3333435058594)
判断凹凸可以通过计算二阶导确定，$f''(x) \ge 0$是凸函数或hessian矩阵半正定：$\nabla^2f(x)\succcurlyeq 0$
【hessian矩阵-矩阵二阶导】
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704242444472-fe2a3ace-2b94-4e97-8a00-04cef8aa90f5.png#averageHue=%23fefefe&clientId=u11c3f106-9eb4-4&from=paste&height=342&id=u8682c1d7&originHeight=513&originWidth=661&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=32606&status=done&style=none&taskId=u87ece63a-4d8e-4141-9d04-c189e03b4f7&title=&width=440.6666666666667)
【jacob矩阵】
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704242470613-b15a1ee6-6f3d-4748-8096-e74cabb9cbd0.png#averageHue=%23fefefe&clientId=u11c3f106-9eb4-4&from=paste&height=219&id=ub1584990&originHeight=328&originWidth=865&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=25779&status=done&style=none&taskId=u694ee665-8071-442a-b46a-a8899e904c8&title=&width=576.6666666666666)
### 6.1.4 概率
#### 6.1.4.1 概率函数

- 概率密度函数（PDF）：连续值取某个值的概率
- 概率质量函数（PMF）：离散值，正好等于某个值的概率
#### 6.1.4.2 期望
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704242966836-4cea70d3-62dd-4d31-a650-a13aad19b695.png#averageHue=%23f7f7f7&clientId=u11c3f106-9eb4-4&from=paste&height=275&id=uda109d93&originHeight=412&originWidth=1102&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=84285&status=done&style=none&taskId=ucf0acc7c-5a52-4c49-bf8d-4b3ff1c4dc2&title=&width=734.6666666666666)
### 6.1.5 jensen不等式
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704243095807-36cc4d11-d308-4a5a-83a9-5d61207d50e8.png#averageHue=%23f8f7f7&clientId=u11c3f106-9eb4-4&from=paste&height=465&id=u6aa9b6a4&originHeight=697&originWidth=1009&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=138864&status=done&style=none&taskId=u7325de29-9216-477d-8b55-bc21ea78b5d&title=&width=672.6666666666666)
### 6.1.6 高斯分布/正态分布
$X \sim N(\mu,\sigma^2)$
概率密度函数：$p(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$
$\mu = 0, \sigma = 1$是标准正态分布
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704243376849-19de419a-58de-485b-aa49-bf8d5a578e2a.png#averageHue=%23f8f7f7&clientId=u11c3f106-9eb4-4&from=paste&height=283&id=uc8b119c5&originHeight=424&originWidth=760&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=94164&status=done&style=none&taskId=uccb2f8db-f9bc-401f-ad80-4b27154c19d&title=&width=506.6666666666667)
【**多变量高斯分布】**
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704243424250-3e5fdff6-1eec-4d9b-8b09-02e5a5159ed5.png#averageHue=%23f5f4f3&clientId=u11c3f106-9eb4-4&from=paste&height=362&id=u1708eadd&originHeight=543&originWidth=999&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=162443&status=done&style=none&taskId=u8bbd9569-e314-4526-a746-9440ca189e7&title=&width=666)
## 6.2 高斯混合模型GMM
多个高斯分布的加权和，利用此模型进行聚类，每一个子分布都是高斯分布
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704243949906-9411b378-6c6a-4ba4-a303-ad5fb3ae4db8.png#averageHue=%23f9f9f8&clientId=u11c3f106-9eb4-4&from=paste&height=395&id=u9729bdac&originHeight=592&originWidth=1083&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=146838&status=done&style=none&taskId=ud5514d27-0e29-4fb9-857d-f0fb6cb6566&title=&width=722)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704244827574-9c07a893-28eb-425e-a86c-98bae895eabf.png#averageHue=%23f8f6f6&clientId=u11c3f106-9eb4-4&from=paste&height=425&id=u5b66f284&originHeight=637&originWidth=958&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=151280&status=done&style=none&taskId=u437939bc-869e-451d-9bcc-ac378be9069&title=&width=638.6666666666666)
## 6.3 最大似然估计MLE
### 6.3.1 定义
maximum likelihood estimation，最大化似然函数以估计概率分布
【单高斯模型】
$L(\theta | X) =  p (X | \theta) = \prod_{j=1}^M p(x_j|\theta)$
两边取对数
### 6.3.2 期望最大化算法EM
#### 6.3.2.1 核心思想
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704245444555-12fa7a40-ed43-430f-818a-ac23d6d830c7.png#averageHue=%23eee9e7&clientId=u11c3f106-9eb4-4&from=paste&height=243&id=u7c70c4fb&originHeight=364&originWidth=1069&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=210688&status=done&style=none&taskId=ud76fff3e-a35b-46f3-86a3-442e763901b&title=&width=712.6666666666666)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704381298277-8c8acfe5-1a60-425f-95c0-47e971889720.png#averageHue=%23faf8f7&clientId=u1cfd4743-4f7a-4&from=paste&height=327&id=u48c32b5d&originHeight=490&originWidth=1117&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=140100&status=done&style=none&taskId=u82b7c835-3caf-4aa2-bd0d-98674728838&title=&width=744.6666666666666)
M-step：使用最大似然估计得到更好的参数$\theta$
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704381580460-c1e43ca5-e83f-47f3-9198-34f40f43d350.png#averageHue=%23f5f4f4&clientId=u1cfd4743-4f7a-4&from=paste&height=393&id=u37ae03fd&originHeight=589&originWidth=969&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=165474&status=done&style=none&taskId=u7a6d87bb-b746-468e-8c91-3d05f4864b3&title=&width=646)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704381646399-ad797dcc-c0f6-47a4-81d0-38c765b15df0.png#averageHue=%23f5edd8&clientId=u1cfd4743-4f7a-4&from=paste&height=409&id=uf7e5993d&originHeight=613&originWidth=1183&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=265488&status=done&style=none&taskId=u2e0f536b-df6a-4524-9597-c248a30c851&title=&width=788.6666666666666)
# 7 SVM
找到能够最大化不同类别数据间隔的超平面，通过最大化决策边界和支持向量的距离提高模型的泛化能力，将数据映射到高维空间中实现线性可分
## 7.1 间隔与支持向量
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704246165540-92d279de-7f4e-48dd-bd8a-af639693ae5f.png#averageHue=%23f9f7f6&clientId=u11c3f106-9eb4-4&from=paste&height=493&id=u656677ea&originHeight=739&originWidth=1147&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=200230&status=done&style=none&taskId=u8d629cc3-21ff-4069-a96d-aec8164668a&title=&width=764.6666666666666)
【间隔】每个样本点到分界超平面的垂直距离
【支持向量】所有样本中拥有最小间隔的点
【SVM目标】最大化最小间隔
## 7.2 计算
### 7.2.1 点到法平面距离
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704246458529-f27a7560-06d2-4c99-bf43-7cdfb753ddd1.png#averageHue=%23f7f5f4&clientId=u11c3f106-9eb4-4&from=paste&height=348&id=u5bf482ca&originHeight=522&originWidth=1132&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=211823&status=done&style=none&taskId=u775ea354-8465-45bc-90e0-ed04f2f5643&title=&width=754.6666666666666)
f(x)>0是正类，反之负类。判断分类预测的正误可以使用yf(x)>0来确认，<0表示分类不统一，错误
### 7.2.2 优化

- 只需要方向，不需要大小，令$||w||=1$
- 限制$min(y_if(x_i)) = min(y_i (w^Tx_i+b)) = 1$（最优解除以任意非0倍数依然是最优解），此时相当于最大化$\frac{1}{||w||}$，即最小化$\frac{1}{2}w^Tw$（加1/2为了消除求导平方系数）
### 7.2.3 求解
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704247513799-7933e49b-8ad1-45c6-8a90-cfdafe1a40bc.png#averageHue=%23faf8f8&clientId=u11c3f106-9eb4-4&from=paste&height=416&id=u3c165688&originHeight=624&originWidth=1000&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=153439&status=done&style=none&taskId=u289fef1a-025e-4e6b-8159-2e07f7a18c0&title=&width=666.6666666666666)
#### 7.2.3.1 KKT条件
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704247946483-f8bdc631-0735-4592-9d6f-0579b2d3e86e.png#averageHue=%23faf8f8&clientId=u11c3f106-9eb4-4&from=paste&height=445&id=ufa1dff4e&originHeight=667&originWidth=1093&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=159685&status=done&style=none&taskId=u2bf1b48e-e34f-418d-9e08-b5d68b736e0&title=&width=728.6666666666666)
#### 7.2.3.2 对偶
引入拉格朗日乘子$a_i$后，该乘子构成对偶空间
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704248036562-8ea4d2b9-cdbd-47f2-ad13-4ae3d5edfd45.png#averageHue=%23fcfbfb&clientId=u11c3f106-9eb4-4&from=paste&height=191&id=u77e67fb3&originHeight=286&originWidth=1002&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=59036&status=done&style=none&taskId=u90e3daa6-9a4c-48dd-bc63-48e28cf51d8&title=&width=668)
#### 7.2.3.3 最优解
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704248154157-f3f1873e-4119-421f-a8c6-92a97ed5410a.png#averageHue=%23faf9f9&clientId=u11c3f106-9eb4-4&from=paste&height=371&id=uf1b62d4f&originHeight=556&originWidth=1023&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=153722&status=done&style=none&taskId=u8b326fb3-edeb-4789-9302-2d9420bfe9e&title=&width=682)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704248178406-f3d233c7-b955-49c4-8a4a-b31446dfcc2d.png#averageHue=%23fbf9f9&clientId=u11c3f106-9eb4-4&from=paste&height=383&id=u8e8fa6c4&originHeight=574&originWidth=961&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=124528&status=done&style=none&taskId=u1271a148-38e8-403f-8fff-ae1d6562dac&title=&width=640.6666666666666)
## 7.3 soft margin
惩罚：松弛变量
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704248279370-2d8cddc0-3e7d-4010-a558-a13833190700.png#averageHue=%23f9f8f7&clientId=u11c3f106-9eb4-4&from=paste&height=367&id=u4f1e5f83&originHeight=550&originWidth=1047&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=193853&status=done&style=none&taskId=ud54c1f0a-5247-4a2b-a13a-251d89ce2eb&title=&width=698)
## 7.4 非线性SVM
映射到更高维的特征空间使得样本线性可分
### 7.4.1 kernel trick
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704248571064-ef1d2c5b-b461-4d84-9fe1-f822a94f86a8.png#averageHue=%23f6f5f3&clientId=u11c3f106-9eb4-4&from=paste&height=350&id=uc7d3827b&originHeight=525&originWidth=1036&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=210383&status=done&style=none&taskId=uc8add346-e987-403b-8d75-995f81974ad&title=&width=690.6666666666666)
### 7.4.2 Mercer's condition
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704248682735-38f9cdb2-4b94-4838-82f7-6256a3c593be.png#averageHue=%23f7f5f3&clientId=u11c3f106-9eb4-4&from=paste&height=302&id=u3ed4b1a7&originHeight=453&originWidth=1062&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=193394&status=done&style=none&taskId=u9284b88b-2024-4edd-b28d-ce482dd1033&title=&width=708)
### 7.4.3 Kernel SVM
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704249047988-47c03703-47ab-4569-a99e-bcd8315b1e08.png#averageHue=%23f6f4f2&clientId=u11c3f106-9eb4-4&from=paste&height=385&id=u65d87cfd&originHeight=577&originWidth=988&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=199604&status=done&style=none&taskId=u6f7d80ea-1839-4e21-b963-4db5e62ad57&title=&width=658.6666666666666)
测试时间为O(nd)
#### 7.4.3.1 线性核
$K(x,y) = x^T y$
#### 7.4.3.2 RBF/Gaussian核
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704249384746-e894ab74-d7d9-43cc-84cf-930e7ec7e29e.png#averageHue=%23efeeec&clientId=u11c3f106-9eb4-4&from=paste&height=81&id=u696ec210&originHeight=121&originWidth=751&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=39423&status=done&style=none&taskId=ua33b5132-7031-4b99-b355-f55b13acf98&title=&width=500.6666666666667)
#### 7.4.3.3 多项式核
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704249409012-9a9cb04c-8150-4423-90d8-2715a9d98578.png#averageHue=%23f3f1f0&clientId=u11c3f106-9eb4-4&from=paste&height=59&id=uee00fc25&originHeight=88&originWidth=391&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=13684&status=done&style=none&taskId=ubec03e3a-94c2-48e9-ba4f-e1ca1b58a7c&title=&width=260.6666666666667)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704249518616-43983e3c-c5b9-49d7-a2eb-c3960c038e27.png#averageHue=%23f8f5f5&clientId=u11c3f106-9eb4-4&from=paste&height=472&id=ud5080357&originHeight=708&originWidth=1057&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=154422&status=done&style=none&taskId=uaaeda80a-fc29-4e31-8ca7-9b5cc8efcda&title=&width=704.6666666666666)
【超参数】
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704249502258-d5ba2f56-e447-4f58-947b-cbf7b0bc9a3d.png#averageHue=%23f7f5f4&clientId=u11c3f106-9eb4-4&from=paste&height=303&id=ucfecd9dd&originHeight=454&originWidth=1063&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=191753&status=done&style=none&taskId=u040b6c79-7520-469a-9c72-c977bf0dd25&title=&width=708.6666666666666)
## 7.5 多类SVM
### 7.5.1 1V1
转化为两类问题，构造$C_n^2$个分类器
将一个样本得到的所有结果按照投票法做决定
### 7.5.2 1 V all
共n个类，设置n个分类器，每个分类器用类i做正类，其他所有类做负类
每个分类器采用实际值输出作为信心，选择信心最高类
# 8 神经元与感知机
## 8.1 Hebbian Theory赫布理论
连接强度的调整量和输入输出的乘积成正比，经常出现的模式将增强神经元之间的连接
又称长度增强机制（LTP：Long Term Potentiation）或神经可塑（Neural Plasticity）
## 8.2 MP神经元
### 8.2.1 基本工作原理
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704290638932-0ee33e10-b189-4854-afbf-37b64df5ca61.png#averageHue=%23f0f0f0&clientId=uee44fb03-d81d-4&from=paste&height=141&id=u9205ea46&originHeight=211&originWidth=508&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=23485&status=done&style=none&taskId=u08c5a3c5-3e97-4e6f-96d6-5e4ff1735ee&title=&width=338.6666666666667)
输入 -》 权值 -》 激活函数
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704290750702-4be51ad0-82eb-441a-a4a7-32fd60ce60ed.png#averageHue=%23faf9f9&clientId=uee44fb03-d81d-4&from=paste&height=456&id=ua8d301ad&originHeight=684&originWidth=1108&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=188857&status=done&style=none&taskId=uc5221662-5ba3-47bc-af0e-063d5af07c2&title=&width=738.6666666666666)
### 8.2.2 局限
输入：线性求和
输出：单一输出值
更新：时钟同步更新
### 8.2.3 激活函数
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704290929368-a09f7cb3-9f98-48be-b3b3-4c339e6fe377.png#averageHue=%23faf6f6&clientId=uee44fb03-d81d-4&from=paste&height=479&id=u8fc2c09d&originHeight=718&originWidth=1038&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=189170&status=done&style=none&taskId=u55a6ec55-c6f1-4f17-93a7-e73d35f9b8b&title=&width=692)
【sigmoid】
饱和激活函数（tanh）$\lim_{n \to +\inf}h'(x) = \lim_{n\to-\inf}h'(x) = 0$饱和激活函数
导数始终小于1，在0周围变化，容易造成梯度消失问题
指数的计算代价大
【ReLU】
非饱和激活函数可以解决梯度消失问题，加快模型收敛速度
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704291289160-443aa5b2-62c5-4603-aeae-d72accd31a3d.png#averageHue=%23f8f7f7&clientId=uee44fb03-d81d-4&from=paste&height=372&id=u51d5a8c8&originHeight=558&originWidth=1128&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=129964&status=done&style=none&taskId=u904e052f-bb22-4f2b-b419-279f5d70ed0&title=&width=752)
## 8.3 感知机
最简单形式的前馈式人工神经网络
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704292415394-178ad335-3a82-4e5b-85bb-f3690795df7e.png#averageHue=%23f9f7f7&clientId=uee44fb03-d81d-4&from=paste&height=435&id=u7eadf653&originHeight=652&originWidth=991&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=134018&status=done&style=none&taskId=ua39a1895-e652-4bbb-9fbe-ec69582427c&title=&width=660.6666666666666)

- 同层内无互联，不同层间没有反馈
- 输入输出均为离散值
- 由阈值函数决定输出

c：学习率，d：期望输出（1或-1），signal：感知机输出
【偏置单元$x_0$】取常数，通常为1
【重要计算】根据样本序列调整权重参数
### 8.3.1 感知机学习算法

1. 权值初始化
2. 输入样本对
3. 计算输出
4. 根据学习规则调整权重
5. 返回步骤2，接着输入下一对样本，直到所有样本的实际输出与期望输出相等
### 8.3.2 线性可分性
【感知机收敛理论】定义$\gamma$是分离超平面和最接近的数据点之间的距离，迭代次数的界是$\frac{1}{\gamma^2}$（前提是样例是线性可分的）
单层神经网络不能解决非线性可分问题：如异或
用多层网络处理异或（二层神经网络可以表达所有的布尔函数）
# 9 神经网络
## 9.1 多层感知机MLP
又称BP神经网络，back propagation
### 9.1.1 基本结构
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704330543169-1d4bfd3a-9da8-418d-bcaa-1be8524ae8e9.png#averageHue=%23fbfbfb&clientId=u5008a986-3ab3-4&from=paste&height=473&id=u4f50aa77&originHeight=709&originWidth=1038&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=151139&status=done&style=none&taskId=u98127244-0fdf-48da-b7dc-5a39414e6ea&title=&width=692)

- 为什么要使用隐藏层？隐藏层是特征检测算子，隐藏层用于发现并刻画数据的特征，可以帮助模型学习非线性和更复杂的模式。
- 如何更新隐藏层的权值？**前向+反向**
### 9.1.2 反向传播
误差反传算法BP
#### 9.1.2.1 误差定义

- 经典感知机（N=1）：$\Sigma_{k=1}^N E_k = \Sigma_{k=1}^N (y_k-t_k)$
- 多层感知机（BP神经网络）：$E = \frac{1}{2}\Sigma_{k=1}^N (y_k-t_k)^2$ 1/2非必须
#### 9.1.2.2 Delta规则
基于误差平面，激活函数必须是**连续的可微分的**
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704331603693-887160f3-81d1-4e55-bbfb-861900121681.png#averageHue=%23f7f6f6&clientId=u5008a986-3ab3-4&from=paste&height=239&id=u421dd312&originHeight=358&originWidth=1024&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=83478&status=done&style=none&taskId=u90028b8d-9290-4094-bd73-6b68d96582c&title=&width=682.6666666666666)
误差定义中，$t_k$可以按照$f(W_k^T · X_k)$计算
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704331905560-838fea62-ccf2-4325-b394-d5ea946cae77.png#averageHue=%23f9f6f5&clientId=u5008a986-3ab3-4&from=paste&height=346&id=ucb90c66e&originHeight=519&originWidth=1038&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=127728&status=done&style=none&taskId=u2bf4a867-bebe-41bf-9393-3a6c244fb25&title=&width=692)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704331969748-2a56013d-a087-4c5a-94c4-ed7270a20866.png#averageHue=%23f7f5f4&clientId=u5008a986-3ab3-4&from=paste&height=344&id=u3f7df104&originHeight=418&originWidth=841&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=96914&status=done&style=none&taskId=u2c256b55-8451-42f4-b9f2-0aa0486d7d9&title=&width=692.6666870117188)
【学习常数c的影响】
c决定学习过程中权值变化快慢
c过大，容易越过最优值或在最优值附近震荡
#### 9.1.2.3 反向传播算法BP
前向：权值固定，输入信号经过网络正向一层层传播，达到输出端
反向：比较输出和期望输出，产生误差信号，误差信号通过网络反向一层层传播，对每一层权值进行修正
【信用分配难题】如何给隐藏层神经元分配信用或责任？
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704332969374-345b33d7-530e-4c15-9323-b9a5329df6f9.png#averageHue=%23f8f8f7&clientId=u5008a986-3ab3-4&from=paste&height=419&id=u8e9d476c&originHeight=628&originWidth=1144&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=181600&status=done&style=none&taskId=u28ef0d36-30ae-4e1f-8e73-f11c1a9c3d7&title=&width=762.6666666666666)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704333009116-846c5f07-a677-4226-a7cf-b4b2884cacfa.png#averageHue=%23fbf6f5&clientId=u5008a986-3ab3-4&from=paste&height=245&id=u78f9f5b9&originHeight=367&originWidth=1065&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=95106&status=done&style=none&taskId=u6b7a1139-72e6-40d0-bb12-a05b889de3f&title=&width=710)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704333068720-2f8192f2-1c20-4173-94c2-8d2c671697bb.png#averageHue=%23fbf8f8&clientId=u5008a986-3ab3-4&from=paste&height=293&id=u6435e7f5&originHeight=439&originWidth=1086&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=119088&status=done&style=none&taskId=u746c92e5-b807-42fa-b22a-263aaa3ae6f&title=&width=724)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704333135535-1d80313c-5f29-4284-b28f-6d2eda8c03ea.png#averageHue=%23faf3f3&clientId=u5008a986-3ab3-4&from=paste&height=232&id=u67fa6669&originHeight=348&originWidth=774&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=74310&status=done&style=none&taskId=ue87148f3-475d-45e7-8d1a-2ce63399012&title=&width=516)
【权值修正的两种情况】

- 神经元j是输出层节点：直接使用delta规则
- j是隐藏层：没有指定输出

![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704333511361-2303c4c1-6dcf-48e0-972b-6a8fa9292497.png#averageHue=%23f5efee&clientId=u5008a986-3ab3-4&from=paste&height=81&id=u03b8d41d&originHeight=121&originWidth=1077&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=37015&status=done&style=none&taskId=u14ad4f4e-4458-4c8e-90fa-b951e10a11b&title=&width=718)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704333523049-f6202f2b-d583-4387-a913-fc5d8316c169.png#averageHue=%23f5f3f2&clientId=u5008a986-3ab3-4&from=paste&height=89&id=u4842decd&originHeight=133&originWidth=636&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=23942&status=done&style=none&taskId=u3e9e8fab-387c-49de-acf8-043b4059f39&title=&width=424)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704333689624-ce677a1e-9df9-4ff7-bd9c-c2042fbad890.png#averageHue=%23f8f3f3&clientId=u5008a986-3ab3-4&from=paste&height=587&id=ubbc098b8&originHeight=880&originWidth=1090&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=212400&status=done&style=none&taskId=u32bd9ce0-fb27-4a76-b761-5c4472e9fc3&title=&width=726.6666666666666)
隐藏层到输出层的梯度d1以error*f'直接计算
输入层到隐藏层的梯度d2 = f'*(所有d1*w的和）计算
#### 9.1.2.4 梯度下降

- 随机梯度下降
- 批量梯度下降（mini-batch随机梯度下降）

**局部梯度域**是指在神经网络中，对于每个特定的参数或权重，它所在的损失函数的梯度的局部环境。换句话说，局部梯度域描述了在参数空间中某一点附近损失函数的变化情况。
### 9.1.3 影响因素
#### 9.1.3.1 初始权值
使用正态分布，$w\sim (0,1)$，输出值~（0，n）
独立变量和的方差 = 独立变量方差和
如果初始权值**过大**，会导致输出值过大，激活函数饱和，梯度消失（学习失效）
#### 9.1.3.2 顺序和批量训练

- 批量训练：计算所有样本误差和（计算代价大、收敛速度快、局部极小）
- 顺序训练：按次序计算每个样本的误差（局部极小、噪声敏感）
- 小批量训练：适合大规模数据

【冲量】
加大搜索步长的效果，越过狭窄的局部极小值
#### 9.1.3.3 停止机制

- 设定固定**迭代步数**
- 设定误差小于固定**阈值**
- 两者结合

利用验证集选择合适参数
## 9.2 自动编码器
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704335111454-8b0ddead-93eb-4434-a752-213085ba6633.png#averageHue=%23f9f8f5&clientId=u5008a986-3ab3-4&from=paste&height=405&id=ua98b498d&originHeight=607&originWidth=1092&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=182034&status=done&style=none&taskId=u72792935-a016-45ec-bf55-f3153c63a4d&title=&width=728)
结构：编码器+解码器
高维到低维，减少神经元数量；低维到高维，增加
前向 -> 重构 -> 计算损失 -> 反向
## 9.3 径向基网络RBF
### 9.3.1 感受野
Receptive Field
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704335233458-544c1360-1e59-4e93-8b6b-7e15f5f86351.png#averageHue=%23fbf8f8&clientId=u5008a986-3ab3-4&from=paste&height=423&id=u67a010e3&originHeight=634&originWidth=1036&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=313316&status=done&style=none&taskId=uce1d85cf-5478-4e8a-aa27-2a7d2f64943&title=&width=690.6666666666666)
激活程度/输出 和 输入数据和权值向量的距离 成比例
### 9.3.2 激活规则
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704335471228-7758f1cd-ae06-4fa0-a4e4-435c8d7c59ee.png#averageHue=%23fbfbfb&clientId=u5008a986-3ab3-4&from=paste&height=442&id=u1e8d6dc8&originHeight=663&originWidth=1057&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=262357&status=done&style=none&taskId=u4862239d-f557-4f7a-aace-7d065a6ba4e&title=&width=704.6666666666666)

- 输入空间 -> 隐藏层：非线性变换（核技巧）
- 隐藏层 -> 输出层：线性变换（和MLP相同）
### 9.3.3 学习规则
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704335615760-2cd734e4-33ab-414b-9927-4274494186be.png#averageHue=%23f7f7f7&clientId=u5008a986-3ab3-4&from=paste&height=365&id=udf25592b&originHeight=547&originWidth=1096&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=138052&status=done&style=none&taskId=u68336940-480b-4d7b-88ef-d0ec8fbae2a&title=&width=730.6666666666666)
求解径向基中心$c_i$，隐藏层到输出层的权值$W_{ij}$
【算法步骤】
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704335791634-7d99db7d-86a2-41e6-a29d-baace6224666.png#averageHue=%23f2f1ec&clientId=u5008a986-3ab3-4&from=paste&height=269&id=u9254d87b&originHeight=403&originWidth=987&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=150917&status=done&style=none&taskId=u672b1c38-f85c-4bae-81e3-c589055af8c&title=&width=658)
### 9.3.4 重要原理
**隐含层的作用**：把低维度的p映射到高维度的h，变成高维线性可分（核函数的思想）
【优点】

- 输入到输出非线性，但是输出可调参数是线性的（权值）
- 网络的权可以由线性方程组直接解出
### 9.3.5 RBF与其他模型对比
#### 9.3.5.1 RBF VS MLP

- 局部逼近VS全局逼近：BP全局逼近，RBF局部逼近
- 中间层数：RBF只能有一个隐含层，BP可以有多个隐含层
- 训练速度：RBF训练速度快
- 最优性：RBF是连续函数的最佳逼近
#### 9.3.5.2 RBF VS SVM
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704336266257-60cab565-c497-417c-9664-cd9ddd76a589.png#averageHue=%23f7f6f5&clientId=u5008a986-3ab3-4&from=paste&height=403&id=u85176081&originHeight=604&originWidth=1042&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=189820&status=done&style=none&taskId=u7b5a6e69-4ebf-4ef4-adca-849e830f209&title=&width=694.6666666666666)
# 10 CNN
## 10.1 卷积

- 全连接
- 局部连接
### 10.1.1 超参数

- depth：number of filters
- stride
- zero-padding

$3 \times 3 \times 3 \times 96$
3*3：核大小，感受野
3：3个通道（彩色图像）
96：深度，层数，过滤器数目
### 10.1.2 池化
下采样
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704337923902-68c90ce3-db71-4b31-a7b4-bab57470c2a7.png#averageHue=%23e5d9c6&clientId=u5008a986-3ab3-4&from=paste&height=213&id=u88d97d76&originHeight=319&originWidth=669&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=101622&status=done&style=none&taskId=u8efaa08c-bebd-4df4-b9a3-ee28764b561&title=&width=446)
【作用】

- 减少参数
- 避免过拟合
- 扩大感受域
### 10.1.3 全连接层
用于全局特征提取（softmax是分类层/头）
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704338023380-d4cdba23-2821-4e53-8adf-594e64300578.png#averageHue=%23f9f9f7&clientId=u5008a986-3ab3-4&from=paste&height=280&id=u79fe6d63&originHeight=420&originWidth=841&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=224287&status=done&style=none&taskId=u178639a0-ada1-4005-93ac-96286b3f19e&title=&width=560.6666666666666)
### 10.1.4 Dropout/Regularization
对大的神经网络进行平均
破坏全连接，以$\rho$的概率保留该神经元
所有模型架构共享权重 -》 每个模型都受到了非常强的正则化约束
### 10.1.5 batch normalization
减少内部方差偏移或避免梯度扩散
### 10.1.6 损失函数

- 交叉熵损失

![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704339363334-1286cc6d-98ec-4d8e-a767-411a2c875fad.png#averageHue=%23fbfafa&clientId=u5008a986-3ab3-4&from=paste&height=129&id=u853442ed&originHeight=193&originWidth=1009&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=51813&status=done&style=none&taskId=ue9601ce3-dfa5-4e8c-b19c-2992a81e7f2&title=&width=672.6666666666666)

- L2-norm

![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704339435606-0f5e4231-2b0c-467d-9de3-1c202e3ff599.png#averageHue=%23faf9f9&clientId=u5008a986-3ab3-4&from=paste&height=186&id=u31b10356&originHeight=279&originWidth=907&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=46110&status=done&style=none&taskId=u7f38768f-5f17-4aa5-8cea-3c319ab09ec&title=&width=604.6666666666666)

- 三元组损失

## 10.2 深度学习的一些技巧
# 11 演化学习
## 11.1 遗传算法GA
基本思想：通过对当前最好的假设模型**重组**来产生后续假设模型
### 11.1.1 基本概念
#### 11.1.1.1 染色体
【一个合取/析取的表示实例】
有两种属性，对应**是否**去打网球两种结果

- outlook：sunny/overcast/rain
- wind：strong/weak

![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704352238678-7d2e5ad4-5f4e-4dc4-bf41-4887d49e5bca.png#averageHue=%23faf4f4&clientId=u5008a986-3ab3-4&from=paste&height=427&id=u15840ae2&originHeight=640&originWidth=1096&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=190116&status=done&style=none&taskId=u32866ff4-ee9c-414d-a3b1-ecebf810850&title=&width=730.6666666666666)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704352263649-36c524f4-8a46-4026-8a53-3e02886b0f22.png#averageHue=%23f7f3f2&clientId=u5008a986-3ab3-4&from=paste&height=233&id=u403fcfbf&originHeight=349&originWidth=663&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=71821&status=done&style=none&taskId=u86ba8e4e-51bb-4f36-86fe-42d2543d0eb&title=&width=442)
【染色体表示】01位串
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704352653859-8aef9bf5-2fe1-4ff8-ad83-3a5a8745b528.png#averageHue=%23f5f3f2&clientId=u5008a986-3ab3-4&from=paste&height=384&id=ue75beaf8&originHeight=576&originWidth=1102&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=245015&status=done&style=none&taskId=udfe0e267-3a19-4d5f-a19f-8dedd228f3e&title=&width=734.6666666666666)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704352823805-d00573b9-6b91-4674-8e49-08e275a4ebfe.png#averageHue=%23f6f2f1&clientId=u5008a986-3ab3-4&from=paste&height=422&id=u9a70384a&originHeight=633&originWidth=1078&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=233248&status=done&style=none&taskId=uaf275440-edfc-4dc3-98ec-81ce32e1876&title=&width=718.6666666666666)
染色体相当于训练数据输入
#### 11.1.1.2 适应度函数/种群
适应度函数：学习的目标函数
种群表示一组染色体和计算的适应度
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704352932609-dfbfe6e2-2597-452d-9a41-6d6b5d5971d5.png#averageHue=%23f7f6f5&clientId=u5008a986-3ab3-4&from=paste&height=255&id=u54dd9657&originHeight=382&originWidth=994&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=84296&status=done&style=none&taskId=u74c09283-be2a-4acd-a3a6-d85b0c5b934&title=&width=662.6666666666666)
#### 11.1.1.3 产生后代
基于适应度函数，有以下几种选择方式：

- **锦标赛选择**：每次选n个（有放回抽样），选择最好的一个进入子代种群（重复直到子代与原来种群规模一致）
- **截断选择**：根据适应度排序，选择前k个；然后复制染色体达到相同的规模
- **轮盘赌选择**：与适应度成比例选择

![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704353208218-b5d504b1-003f-4f99-a7e4-6b2b650e80bb.png#averageHue=%23f3f2f1&clientId=u5008a986-3ab3-4&from=paste&height=100&id=u1f145091&originHeight=150&originWidth=723&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=38022&status=done&style=none&taskId=u518de7ab-b99b-48ee-b849-44dda90328c&title=&width=482)
### 11.1.2 一般形式

1. 初始化种群P(t)：一般来说第0代种群随机生成
2. 如果不满足终止条件
   1. 评估每个染色体适应度
   2. 选择染色体产生后代
   3. 用后代替换染色体并重复这一阶段步骤
3. 终止
### 11.1.3 遗传算子
GA Operator，对当前群体中的染色体进行重组以产生后代
#### 11.1.3.1 交叉crossover
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704353477285-10c1c67a-2142-4b9e-85a4-3e18061101dd.png#averageHue=%23f3eeeb&clientId=u5008a986-3ab3-4&from=paste&height=285&id=ub8898422&originHeight=427&originWidth=1087&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=320936&status=done&style=none&taskId=uc3f20fe4-b8cd-445a-929e-1ab44f79304&title=&width=724.6666666666666)
#### 11.1.3.2 变异
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704353513698-427498d2-51bb-421c-82b7-193d7499514f.png#averageHue=%23f4f0ee&clientId=u5008a986-3ab3-4&from=paste&height=383&id=uf370f614&originHeight=574&originWidth=895&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=227559&status=done&style=none&taskId=u24289b29-ee89-4204-a64b-909c658a5a5&title=&width=596.6666666666666)
### 11.1.4 种群演化

- **简易方案**：后代替代父代（易丢失优秀解）
- **精英法**：保留上一代最优，差的用子代替换（与选择算子混用）
- **锦标赛法**：父代与后代参与竞争，选择胜者
- **小生境法**：先分类，每类选择优秀代表组成群，群内和群间杂交变异产生新一代

新一代选择机制：

   - 预选择：只使用高适应度子代替换父代
   - 排挤：子代与父代模式相似的个体被替换
   - 共享：计算适应度和模式的关联关系，共享这种模式（见模式理论部分）
### 11.1.5 问题

- 编码不规范，表示不准确
- 约束：单一遗传算法编码不能全面表示优化问题的约束
- 搜索效率低，容易过早收敛
- 没有有效定量分析的手段
## 11.2 模式理论
### 11.2.1 基本概念
模式是由0，1，#组成的任意串，#表示既可以是0也可以是1
阶o(H)表示模式中确定位置的个数，如o(##1#0) = 2
长度d(H)表示第一个确定位置到最后一个确定位置的距离d(##1#0) = 2
m(s,t)表示第t代种群中，模式s的实例数量
### 11.2.2 理论
根据GA原理，推断m(s,t+1)的期望值
# 12 维度约简
## 12.1 特征选择和降维

- 特征选择
- 特征诱导/变化

搜索最优特征子集

- 前向：起点为空集
- 后向：起点为全集
- 双向
### 12.1.1 搜索策略

- 穷举
- 序列
- 随机
### 12.1.2 特征评估

- 过滤式：距离度量（方差）、互信息熵、依赖性度量（皮尔逊相关系数、卡方检验）、一致性度量
- 封装式：分类错误率
- 嵌入式：模型正则化+稀疏约束
## 12.2 线性判别分析LDA
linear discriminant analysis
**有监督的聚类**特征降维方法：给定标注类别的高维数据集，投影到低维的超平面
### 12.2.1 目标
投影使得类内越近越好，类间越远越好
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704357723211-cfa9ccd8-5c1e-4874-914a-6393d9418433.png#averageHue=%23f8f8f7&clientId=u5008a986-3ab3-4&from=paste&height=335&id=u8ba0fb41&originHeight=502&originWidth=1117&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=164013&status=done&style=none&taskId=uc585759a-4592-461c-9c15-fe8762e70a1&title=&width=744.6666666666666)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704357789937-3a258ced-9f50-43c2-842e-662b9241f7dd.png#averageHue=%23f7f5f4&clientId=u5008a986-3ab3-4&from=paste&height=234&id=uaec3241d&originHeight=351&originWidth=961&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=79187&status=done&style=none&taskId=u00b57f8a-da45-435b-9bfe-5537e837b06&title=&width=640.6666666666666)
目标是最小化$\frac{w^TS_Ww}{w^TS_Bw}$
### 12.2.2 计算
对目标式w求导，令导数为0，得到极值
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704358015933-434577b4-ef2f-45c1-98ad-d66fb1b4d129.png#averageHue=%23f6f5f5&clientId=u5008a986-3ab3-4&from=paste&height=302&id=u4effcb18&originHeight=453&originWidth=675&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=76789&status=done&style=none&taskId=uc8f89e2c-91df-4d0f-8de4-c6bea91ffca&title=&width=450)
### 12.2.3 算法流程

1. 计算每个类别均值$\mu_i$，全局样本$\mu$
2. 计算类内散度矩阵$S_W$，类间散度矩阵$S_B$
3. 对矩阵$S_W^{-1}S_B$做特征值分解
4. 取数目最多的特征值对应的特征向量作为投影向量
5. 计算投影矩阵
### 12.2.4 例题理解

## 12.3 主成分分析PCA
Principal Component Analysis
**无监督的**特征降维方法
### 12.3.1 主成分

- 最大可分性：样本在第一主成分上的投影离散程度要大于第二主成分投影离散程度
- 最近可重构性：样本到第一主成分线的平均距离要小于到第二主成分线的距离

因此，要最大化样本点在主成分投影上的方差
### 12.3.2 计算推导
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704358974942-848e9a10-e788-4aee-9047-88532d1b825e.png#averageHue=%23f6f6f6&clientId=u5008a986-3ab3-4&from=paste&height=377&id=uf68770f7&originHeight=565&originWidth=1093&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=174446&status=done&style=none&taskId=u2df56c04-69e3-49ab-981d-f2e266b3426&title=&width=728.6666666666666)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704359274658-e5afa1c7-8566-4e73-b9a3-dfaf73e6c229.png#averageHue=%23f8f0ef&clientId=u5008a986-3ab3-4&from=paste&height=463&id=u8dd181e7&originHeight=694&originWidth=1104&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=211169&status=done&style=none&taskId=uf651a197-093a-4376-8af2-fe5c07d5e7b&title=&width=736)
### 12.3.3 算法流程

1. 样本去中心化（一行一个样本）：减去每列的平均值
2. 计算协方差矩阵 $\frac{1}{n} X^T X$
3. 对协方差矩阵特征值分解
4. 取最大特征值对应的特征向量作为投影向量
5. 计算投影矩阵
### 12.3.4 核PCA
投射到高维
### 12.3.5 PCA VS LDA

- PCA无监督，LDA有监督
- PCA投影后数据方差最大，LDA目标组内方差小，组间方差大
- PCA基础是特征的协方差矩阵，投影后更难被分类
- PCA投影后坐标正交，LDA无需正交
- PCA投影后维度数目与源数据相同，LDA投影后数目与类别数目相同
## 12.4 独立成分分析ICA
# 13 强化学习
## 13.1 MDP模型
Markov Decision Process
### 13.1.1 数学模型

- S：states 状态集合
- A：actions 动作集合
- $\delta$：transition probability 状态转移概率
- R：immediate reward 即时奖赏函数
- episode：从初始状态开始经历一系列状态和动作，直到达到终止状态的一次完整实验或轨迹

动态规划过程：利用贪心策略，最大化期望奖赏
### 13.1.2 返回函数
将所有即时奖赏线性组合成一个单一值
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704373190554-2a672fc2-2398-4762-8665-24b56426d11c.png#averageHue=%23f7f4f3&clientId=u1cfd4743-4f7a-4&from=paste&height=381&id=u6f1d6e4b&originHeight=571&originWidth=1077&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=142712&status=done&style=none&taskId=ubbba8ca5-6d35-4545-9495-046eb5b02cc&title=&width=718)
奖赏/状态的分布是策略依赖的
### 13.1.3 动态规划
#### 13.1.3.1 $\epsilon$贪心策略
ϵ的值越小，贪婪行动的概率就越高，而随机行动的概率就越低；反之，ϵ的值越大，随机行动的概率就越高，而贪婪行动的概率就越低。
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704379897280-7797a15c-c5f7-4fb2-8a2b-6b6499e22f67.png#averageHue=%23f3f1f0&clientId=u1cfd4743-4f7a-4&from=paste&height=112&id=uff5abe94&originHeight=168&originWidth=820&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=55172&status=done&style=none&taskId=u8a0e0441-1b36-4177-b50d-3043c4aa55b&title=&width=546.6666666666666)
#### 13.1.3.2 值函数
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704373462400-73e45dae-60c1-4b8b-9a15-a0bf314757c3.png#averageHue=%23f5f1f0&clientId=u1cfd4743-4f7a-4&from=paste&height=403&id=ufcc3f309&originHeight=604&originWidth=1090&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=217305&status=done&style=none&taskId=u34af3d84-1950-4bb9-a6eb-60404d2ff93&title=&width=726.6666666666666)
#### 13.1.3.3 Bellman等式
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704373578885-3692d436-a19a-4fe0-9285-d9a33f7b2d83.png#averageHue=%23f7f4f4&clientId=u1cfd4743-4f7a-4&from=paste&height=406&id=u3887ecd6&originHeight=609&originWidth=1111&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=159315&status=done&style=none&taskId=u56ff9418-fab7-47d9-8da6-d9e3fd71a74&title=&width=740.6666666666666)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704374561842-31c566c6-59ef-4396-83b7-091b6dafcc12.png#averageHue=%23f9f4f3&clientId=u1cfd4743-4f7a-4&from=paste&height=179&id=u2d5f3896&originHeight=268&originWidth=1081&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=67896&status=done&style=none&taskId=u0cc2c82e-7aad-4b21-9afc-1ecaf604bdf&title=&width=720.6666666666666)
### 13.1.4 计算最优策略
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704374753386-944fa37a-0c06-4da6-a543-0642311c7496.png#averageHue=%23fbf9f8&clientId=u1cfd4743-4f7a-4&from=paste&height=281&id=u45486dd3&originHeight=421&originWidth=861&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=81149&status=done&style=none&taskId=u3bce2c0c-20dd-44d0-95d6-679e4fa3d90&title=&width=574)
## 13.2 强化学习方法
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704374841649-0ab8f8fa-93fb-4837-aa3e-2052e3285a68.png#averageHue=%23f6f5f5&clientId=u1cfd4743-4f7a-4&from=paste&height=430&id=u0191f4bc&originHeight=645&originWidth=1141&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=208372&status=done&style=none&taskId=u1ca3c8bb-d65f-402f-8e81-247e09ea5d5&title=&width=760.6666666666666)
### 13.2.1 Monte Carlo
#### 13.2.1.1 策略评价
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704375071608-1f2ee23c-12ef-4491-8b73-1ae0948f7e51.png#averageHue=%23f7f5f4&clientId=u1cfd4743-4f7a-4&from=paste&height=445&id=uf4dec3cd&originHeight=667&originWidth=1042&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=194944&status=done&style=none&taskId=u8f7fa112-480c-4ad1-a03c-6aa94832305&title=&width=694.6666666666666)

#### 13.2.1.2 最优控制
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704375098490-6674b6fa-456a-45a0-9539-8ea52efe2a37.png#averageHue=%23f8f5f5&clientId=u1cfd4743-4f7a-4&from=paste&height=443&id=uc143c50e&originHeight=664&originWidth=1063&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=168066&status=done&style=none&taskId=u9d04759f-b720-4056-aba1-a647ff3a53e&title=&width=708.6666666666666)
控制整个学习过程
#### 13.2.1.3 MC方法
蒙特卡罗方法：不通过估计值更新，通过经验更新（区别于时差学习）-- 采样
N步回退学习？
【TD($\lambda$)算法】
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704375630882-eaf6191c-684b-4813-964f-47b3bb90fdb7.png#averageHue=%23faf9f9&clientId=u1cfd4743-4f7a-4&from=paste&height=447&id=u244c1cad&originHeight=670&originWidth=886&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=137609&status=done&style=none&taskId=u6f76a254-aad4-41a9-8df7-68281d2a209&title=&width=590.6666666666666)
#### 13.2.1.4 时差学习/方法
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704375325625-f8d6dcf2-24ce-4500-8c1a-b526e6817156.png#averageHue=%23fbfafa&clientId=u1cfd4743-4f7a-4&from=paste&height=387&id=ucf0d47e5&originHeight=580&originWidth=1033&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=148650&status=done&style=none&taskId=u75b09da8-3ac6-4982-b59b-7c5f3c32519&title=&width=688.6666666666666)
通过一个估计值进行更新（bootstraps）
#### 13.2.1.5 Sampling和bootstraps的区别
![image.png](https://cdn.nlark.com/yuque/0/2024/png/35670178/1704375781944-0d933077-73bd-4d61-96fd-6fc088b459f2.png#averageHue=%23f9f5f5&clientId=u1cfd4743-4f7a-4&from=paste&height=387&id=u9877099a&originHeight=580&originWidth=927&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=143525&status=done&style=none&taskId=u25bad01f-c666-4d23-ad12-a33a307b06e&title=&width=618)








